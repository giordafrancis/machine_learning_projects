{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced take on wine quality classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests . More on the dataset [here](https://archive.ics.uci.edu/ml/datasets/wine+quality).\n",
    "\n",
    "The objective here is to predict the wine quality based on it's properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data\")\n",
    "path_red = path_data / \"winequality-red.csv\"\n",
    "path_white = path_data / \"winequality-white.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = pd.read_csv(path_red, sep=';')\n",
    "df_white= pd.read_csv(path_white, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red wine classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_white.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_white.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.concat([df_white.assign(white=1), df_red.assign(red=1)]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.hist(figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red['quality'] = np.where(df_red['quality'] > 6, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "    0: 'blue',\n",
    "    1: 'red'\n",
    "}\n",
    "col_map = df_red['quality'].map(color_dict).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_red, diagonal='kde', color=col_map, figsize= (12,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will predict class labels weather a wine is good above 7 quality or not below. \n",
    "Precision and recall are a good metric to start. We want to maximise recall as detection of a good wine is the objective. The F2-measure is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "X, y = df_red.iloc[:, :-1].values, df_red.iloc[:, -1].values\n",
    "print(X.shape, y.shape, Counter(y))\n",
    "model = DummyClassifier(strategy='constant', constant=1)\n",
    "metric = make_scorer(f2_measure)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "print(\"Mean f2: %.3f +/- (%.3f)\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of the F2-measure scores is reported.  In this case, we can see that the baseline algorithm achieves an F2-measure of about 0.440.  This score provides a lower limit on model skill; any model that achieves an average F2-measure above about 0.440 has skill, whereas models that achieve a score below this value do not have skill on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models review generic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def get_models():\n",
    "    models = {}\n",
    "    models['LR'] = LogisticRegression(solver='liblinear')\n",
    "    models['LDA'] = LinearDiscriminantAnalysis()\n",
    "    models['GNB'] = GaussianNB()\n",
    "    models['SVM'] = SVC(gamma='scale')\n",
    "    models['GPC'] = GaussianProcessClassifier()\n",
    "    return models\n",
    "\n",
    "# stratify y\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, stratify=y)\n",
    "\n",
    "results = {}\n",
    "for name, model in get_models().items():\n",
    "    cv =  RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    print(\"%s f2_score %.3f +/- (%.3f)\" % (name, np.mean(scores), np.std(scores)))\n",
    "    results[name] = scores\n",
    "    \n",
    "pyplot.figure(figsize=(10,6));\n",
    "pyplot.boxplot(x= results.values(), labels=results.keys(),\n",
    "               showmeans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As GNB and GPC assume Gaussian inputs and are preforming above the Dummy classifier will perform a power transform to all variables to a Gaussian distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Review transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, model in get_models().items():\n",
    "    cv =  RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    steps = [\n",
    "    ('p', PowerTransformer()),\n",
    "    ('m', model)\n",
    "    ]\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    print(\"%s f2_score %.3f +/- (%.3f)\" % (name, np.mean(scores), np.std(scores)))\n",
    "    results[name] = scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a real improvement overall. Next strategy will be to test come balanced cost - sensitive model to data to verify if any improvement is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model review cost-sensitive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def get_models():\n",
    "    models = {}\n",
    "    models['LR'] = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "    models['GNB'] = GaussianNB()\n",
    "    models['SVM'] = SVC(gamma='scale', class_weight='balanced')\n",
    "    models['GPC'] = GaussianProcessClassifier()\n",
    "    models['DT'] = DecisionTreeClassifier(class_weight='balanced')\n",
    "    return models\n",
    "\n",
    "results = {}\n",
    "for name, model in get_models().items():\n",
    "    cv =  RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    steps = [\n",
    "    ('p', PowerTransformer()),\n",
    "    ('m', model)\n",
    "    ]\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    print(\"%s f2_score %.3f +/- (%.3f)\" % (name, np.mean(scores), np.std(scores)))\n",
    "    results[name] = scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(10,6));\n",
    "pyplot.boxplot(x= results.values(), labels=results.keys(),\n",
    "               showmeans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant boost in scores here. LR and SVM at around .66 and .69. Cost sensitive learning migth be a good route here. \n",
    "Other approaches to review : Ensemble RF with class weighting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote and Tomek links\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def get_models():\n",
    "    models = {}\n",
    "    models['LR'] = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "    models['SVM'] = SVC(gamma='scale', class_weight='balanced')\n",
    "    models['DT'] = DecisionTreeClassifier(class_weight='balanced')\n",
    "    return models\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "results = {}\n",
    "\n",
    "for name, model in get_models().items():\n",
    "    \n",
    "    pipeline = Pipeline(steps=[ \n",
    "                      ('t', PowerTransformer()),\n",
    "                      ('r', resample),\n",
    "                      ('m', model)\n",
    "    ])\n",
    "    results[name] = cross_val_score(pipeline, X_train, y_train, scoring = metric, cv=cv, n_jobs=-1)\n",
    "    print(\"Model %s -> Mean F2 score %.3f +/- (%.3f)\" % (name, np.mean(results[name]),\n",
    "                                                        np.std(results[name])\n",
    "                                                        )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(10,6));\n",
    "pyplot.boxplot(x= results.values(), labels=results.keys(),\n",
    "               showmeans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost sensitive-methods appear more effective so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier, EasyEnsembleClassifier\n",
    "\n",
    "def get_models():\n",
    "    models = {}\n",
    "    models['RF'] = RandomForestClassifier(n_estimators=10)\n",
    "    models['RF_bal'] = RandomForestClassifier(n_estimators=10, class_weight='balanced')\n",
    "    models['RF_bal_sub'] = RandomForestClassifier(n_estimators=10, class_weight='balanced_subsample')\n",
    "    models['B_RF'] = BalancedRandomForestClassifier(n_estimators=10)\n",
    "    models['B_BAG'] = BalancedBaggingClassifier(n_estimators=10)\n",
    "    models['Easy'] = EasyEnsembleClassifier(n_estimators=10)\n",
    "    return models\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "results = {}\n",
    "for name, model in get_models().items():\n",
    "    results[name] = cross_val_score(model, X_train, y_train, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    print(\"Model %s -> Mean F2 score %.3f +/- (%.3f)\" % (name, np.mean(results[name]),\n",
    "                                                        np.std(results[name])\n",
    "                                                        )\n",
    "         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(10,6));\n",
    "pyplot.boxplot(x= results.values(), labels=results.keys(),\n",
    "               showmeans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Ensemble methods seems a good final candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "param_grid = {'n_estimators':[10,100,500,1000]}\n",
    "model = BalancedRandomForestClassifier()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                    scoring=metric, cv=cv)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "model = BalancedRandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "final_f2_score = f2_measure(y_true = y_test, y_pred=y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final model f2_score %.3f\" % (final_f2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(pd.concat([pd.Series(y_test), pd.Series(y_hat)], axis=1))\n",
    "test.columns = ['real', 'pred']\n",
    "test.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scripts] *",
   "language": "python",
   "name": "conda-env-scripts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
